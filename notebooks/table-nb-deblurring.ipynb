{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from notebooks.utils import *\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEX_TABLE_DIR = \"../masters-thesis-tex/tables/\"\n",
    "\n",
    "MODEL_COL = \"model\"\n",
    "METHOD_COL = \"method\"\n",
    "DATASET_COL = \"dataset\"\n",
    "IMAGE_COL = \"image_index\"\n",
    "NOISE_COL = \"noise_stddev\"\n",
    "KERNEL_COL = \"kernel\"\n",
    "METRIC_COL = \"metric\"\n",
    "VALUE_COL = \"value\"\n",
    "\n",
    "METHOD_MAPPING = {\n",
    "    \"DMSP_ORIG\": \"DMSP \\\\cite{bigdeli_deep_2017}\",\n",
    "    \"DPIR\": \"DPIR \\\\cite{zhang_plug-and-play_2021}\",\n",
    "    \"FDN\": \"FDN \\\\cite{kruse_learning_2017}\",\n",
    "    \"dmsp\": \"DMSP\",\n",
    "    \"hqs\": \"HQS\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqs_folder = \"results/nb_deblurring/hqs/\"\n",
    "dmsp_folder = \"results/nb_deblurring/dmsp/\"\n",
    "dmsp_orig_csv = \"results/nb_deblurring/dmsp_paper.csv\"\n",
    "dpir_csv = \"results/nb_deblurring/dpir.csv\"\n",
    "fdn_csv = \"results/nb_deblurring/fdn.csv\"\n",
    "\n",
    "# HQS\n",
    "df_hqs = load_folder(hqs_folder, MODEL_COL)\n",
    "df_hqs[METHOD_COL] = \"hqs\"\n",
    "\n",
    "# DMSP\n",
    "df_dmsp = load_folder(dmsp_folder, MODEL_COL)\n",
    "df_dmsp[METHOD_COL] = \"dmsp\"\n",
    "\n",
    "# DMSP reference code\n",
    "df_dmsp_orig = pd.read_csv(dmsp_orig_csv)\n",
    "df_dmsp_orig[MODEL_COL] = \"\"\n",
    "df_dmsp_orig[METHOD_COL] = \"DMSP_ORIG\"\n",
    "\n",
    "# DPIR\n",
    "df_dpir = pd.read_csv(dpir_csv)\n",
    "df_dpir[MODEL_COL] = \"\"\n",
    "df_dpir[METHOD_COL] = \"DPIR\"\n",
    "\n",
    "# FDN\n",
    "df_fdn = pd.read_csv(fdn_csv)\n",
    "df_fdn[MODEL_COL] = \"\"\n",
    "df_fdn[METHOD_COL] = \"FDN\"\n",
    "\n",
    "# Combine the datasets\n",
    "df_all = pd.concat([df_hqs, df_dmsp, df_dmsp_orig, df_dpir, df_fdn], axis=0)\n",
    "\n",
    "# Read the model name mapping\n",
    "with open(\"models/model_name_mapping.json\") as f:\n",
    "    model_mapping = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the model name mapping\n",
    "with open('models/model_name_mapping.json') as f:\n",
    "    model_mapping = json.load(f)\n",
    "\n",
    "model_mapping = {\n",
    "    \"\": \"\",\n",
    "    **model_mapping\n",
    "}\n",
    "models = list(model_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all[DATASET_COL].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Text Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"BSDS500\"\n",
    "metrics = [\"PSNR\", \"LPIPS_ALEX\"]\n",
    "metrics_mapping = {\n",
    "    \"PSNR\": \"PSNR\",\n",
    "    \"LPIPS_ALEX\": \"LPIPS\",\n",
    "}\n",
    "noise_levels = [0.01, 0.02, 0.03, 0.04]\n",
    "methods = [\"FDN\", \"DMSP_ORIG\", \"DPIR\", \"dmsp\", \"hqs\"]\n",
    "models = [\n",
    "    \"\",\n",
    "    \"dcnn_0.05\",\n",
    "    \"drcnn_0.05\",\n",
    "    \"dunet_0.05\",\n",
    "    \"drunet+_0.05\",\n",
    "    \"drunet+_0.0-0.2\",\n",
    "    # \"drugan+-lambda-zero_0.0-0.2\",\n",
    "    \"drugan+_0.0-0.2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.copy()\n",
    "\n",
    "# Filter only the configured dataset\n",
    "df = df[df[DATASET_COL] == dataset]\n",
    "df = df.drop(DATASET_COL, axis=1)\n",
    "\n",
    "# Mean over the images\n",
    "df = df.groupby([METHOD_COL, MODEL_COL, NOISE_COL]).mean()\n",
    "df = df.drop(IMAGE_COL, axis=1)\n",
    "df = df.reset_index()\n",
    "\n",
    "# Combine all metrics in one column\n",
    "df = df.melt(\n",
    "    id_vars=[METHOD_COL, MODEL_COL, NOISE_COL],\n",
    "    var_name=METRIC_COL,\n",
    "    value_name=VALUE_COL,\n",
    ")\n",
    "\n",
    "# Filter metrics and datasets\n",
    "df = df[df[METRIC_COL].isin(metrics)]\n",
    "df = df[df[MODEL_COL].isin(models)]\n",
    "df = df[df[NOISE_COL].isin(noise_levels)]\n",
    "df = df[df[METHOD_COL].isin(methods)]\n",
    "\n",
    "# Sort by Noise, and Metric\n",
    "df = df.sort_values(METRIC_COL, key=sort_key_for(metrics), kind=\"mergesort\")\n",
    "df = df.sort_values(NOISE_COL, kind=\"mergesort\")\n",
    "\n",
    "# Remap metrics\n",
    "df[METRIC_COL] = df[METRIC_COL].map(metrics_mapping)\n",
    "\n",
    "# Noise Level and Metric as columns\n",
    "df = df.set_index([METHOD_COL, MODEL_COL, NOISE_COL, METRIC_COL])\n",
    "df = df.unstack([NOISE_COL, METRIC_COL])\n",
    "\n",
    "# Sort and remap models\n",
    "df = df.reset_index()\n",
    "df = df.sort_values(MODEL_COL, key=sort_key_for(models), kind=\"mergesort\")\n",
    "df = df.sort_values(METHOD_COL, key=sort_key_for(methods), kind=\"mergesort\")\n",
    "df[MODEL_COL] = df[MODEL_COL].map(model_mapping)\n",
    "df[METHOD_COL] = df[METHOD_COL].map(METHOD_MAPPING)\n",
    "df = df.set_index([METHOD_COL, MODEL_COL])\n",
    "\n",
    "# Organize the column naming\n",
    "df.columns = df.columns.droplevel(0)\n",
    "df.columns = df.columns.rename([\"Additive noise \\( \\sigma_n \\)\", \"\"])\n",
    "\n",
    "# Rename model column\n",
    "df.index = df.index.rename([\"Method\", \"\"])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_format = \"ll\" + \"C{1}\" * len(df.columns)\n",
    "\n",
    "\n",
    "formatters = {\n",
    "    c: mark_column_best_formatter(\n",
    "        df, c, mark_max=c[1] == \"PSNR\", num_decimals=2 if c[1] == \"PSNR\" else 4\n",
    "    )\n",
    "    for c in df.columns\n",
    "}\n",
    "\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    latex = df.to_latex(\n",
    "        # float_format=float_format(True),\n",
    "        formatters=formatters,\n",
    "        escape=False,\n",
    "        column_format=column_format,\n",
    "        multicolumn_format=\"c\",\n",
    "        multirow=True,\n",
    "    )\n",
    "# Use tabularx\n",
    "latex = latex.replace(\"\\\\begin{tabular}\", \"\\\\begin{tabularx}{\\\\textwidth}\")\n",
    "latex = latex.replace(\"\\\\end{tabular}\", \"\\\\end{tabularx}\")\n",
    "\n",
    "# Use multicolumn sometimes\n",
    "latex = latex.replace(\n",
    "    METHOD_MAPPING[\"DMSP_ORIG\"] + \" &\",\n",
    "    \"\\\\multicolumn{2}{l}{\" + METHOD_MAPPING[\"DMSP_ORIG\"] + \"}\",\n",
    ")\n",
    "latex = latex.replace(\n",
    "    METHOD_MAPPING[\"DPIR\"] + \" &\",\n",
    "    \"\\\\multicolumn{2}{l}{\" + METHOD_MAPPING[\"DPIR\"] + \"}\",\n",
    ")\n",
    "latex = latex.replace(\"Method & {}\", \"\\\\multicolumn{2}{l}{Method}\")\n",
    "\n",
    "latex = add_midrule(latex, 9)\n",
    "# latex = add_midrule(latex, 18)\n",
    "\n",
    "# print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEX_TABLE_DIR + \"nb-deblurring_bsds500.tex\", \"w\") as f:\n",
    "    f.write(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_kernels = {\n",
    "    \"BSDS500-schelten\": \"BSDS500~\\cite{arbelaez_contour_2011}\",\n",
    "    \"Set5-levin_1\": \"Set5 + 2nd kernel from~\\cite{levin_understanding_2009}\",\n",
    "    \"Set5-levin_3\": \"Set5 + 4th kernel from~\\cite{levin_understanding_2009}\",\n",
    "}\n",
    "metrics = {\n",
    "    \"PSNR\": \"P\",\n",
    "    \"SSIM\": \"S\",\n",
    "    \"FSIM\": \"F\",\n",
    "    \"LPIPS_ALEX\": \"L\",\n",
    "}\n",
    "noise_levels = [0.01, 0.02, 0.03, 0.04]\n",
    "methods = [\"FDN\", \"DMSP_ORIG\", \"DPIR\", \"dmsp\", \"hqs\"]\n",
    "models = [\n",
    "    \"\",\n",
    "    \"dcnn_0.05\",\n",
    "    \"drcnn_0.05\",\n",
    "    \"dunet_0.05\",\n",
    "    \"drunet+_0.05\",\n",
    "    \"drunet+_0.0-0.2\",\n",
    "    # \"drugan+-lambda-zero_0.0-0.2\",\n",
    "    \"drugan+_0.0-0.2\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.copy()\n",
    "\n",
    "\n",
    "# Combine dataset and kernel\n",
    "df[DATASET_COL] = df[DATASET_COL] + \"-\" + df[KERNEL_COL]\n",
    "# df = df[(df[DATASET_COL] + \"-\" + df[KERNEL_COL]).isin(foo)]\n",
    "\n",
    "# Mean over the images\n",
    "df = df.groupby([METHOD_COL, MODEL_COL, NOISE_COL, DATASET_COL]).mean()\n",
    "df = df.drop(IMAGE_COL, axis=1)\n",
    "df = df.reset_index()\n",
    "\n",
    "# Combine all metrics in one column\n",
    "df = df.melt(\n",
    "    id_vars=[METHOD_COL, MODEL_COL, DATASET_COL, NOISE_COL],\n",
    "    var_name=METRIC_COL,\n",
    "    value_name=VALUE_COL,\n",
    ")\n",
    "\n",
    "# Filter metrics and datasets\n",
    "df = df[df[METRIC_COL].isin(metrics)]\n",
    "df = df[df[DATASET_COL].isin(datasets_kernels)]\n",
    "df = df[df[METHOD_COL].isin(methods)]\n",
    "df = df[df[MODEL_COL].isin(models)]\n",
    "\n",
    "# Remap names\n",
    "df = df.sort_values(MODEL_COL, key=sort_key_for(models), kind=\"mergesort\")\n",
    "df = df.sort_values(METHOD_COL, key=sort_key_for(methods), kind=\"mergesort\")\n",
    "df[METHOD_COL] = df[METHOD_COL].map(METHOD_MAPPING)\n",
    "df[MODEL_COL] = df[MODEL_COL].map(model_mapping)\n",
    "\n",
    "# Models as columns\n",
    "df = df.set_index([DATASET_COL, NOISE_COL, METRIC_COL, METHOD_COL, MODEL_COL])\n",
    "df = df.unstack([METHOD_COL, MODEL_COL])\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "# Sort by Dataset, Noise, and Metric\n",
    "df = df.sort_values(METRIC_COL, key=sort_key_for(metrics), kind=\"mergesort\")\n",
    "df = df.sort_values(NOISE_COL, kind=\"mergesort\")\n",
    "df = df.sort_values(DATASET_COL, key=sort_key_for(datasets_kernels), kind=\"mergesort\")\n",
    "\n",
    "# Rename Dataset, Noise, and Metric\n",
    "df[DATASET_COL] = df[DATASET_COL].map(\n",
    "    lambda x: \"\\rotatebox[origin=c]{90}{\" + datasets_kernels[x] + \"}\"\n",
    ")\n",
    "df[NOISE_COL] = df[NOISE_COL].map(\n",
    "    lambda x: \"\\rotatebox[origin=c]{90}{\\( \\sigma_n \\)=\" + str(x) + \" }\"\n",
    ")\n",
    "df[METRIC_COL] = df[METRIC_COL].map(metrics)\n",
    "\n",
    "df = df.set_index([DATASET_COL, NOISE_COL, METRIC_COL])\n",
    "\n",
    "# Update the column index and index\n",
    "df.columns = df.columns.droplevel(0)\n",
    "df.columns = df.columns.rename([\"\", \"\"])\n",
    "df.index = df.index.rename([\"\", \"\", \"\"])\n",
    "\n",
    "# Sort models\n",
    "# df = df.reindex(list(model_mapping.values()), axis=1)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_format = \"lll|\" + \"C{0.7}C{0.7}C{0.7}|\" + \"C{1}\" * 6 + \"|\" + \"C{1}\" * 2\n",
    "\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    latex = df.to_latex(\n",
    "        float_format=float_format(True),\n",
    "        escape=False,\n",
    "        na_rep=\"---\",\n",
    "        column_format=column_format,\n",
    "        multicolumn_format=\"c\",\n",
    "        multirow=True\n",
    "    )\n",
    "# Use tabularx\n",
    "latex = latex.replace(\"\\\\begin{tabular}\", \"\\\\begin{tabularx}{\\\\textwidth}\")\n",
    "latex = latex.replace(\"\\\\end{tabular}\", \"\\\\end{tabularx}\")\n",
    "\n",
    "# Bug in pandas?? \"&\" Missing in line 4\n",
    "lines = latex.splitlines()\n",
    "lines[3] = \"& & &\" + lines[3]\n",
    "latex = \"\\n\".join(lines)\n",
    "\n",
    "latex = delete_line(latex, 4)\n",
    "\n",
    "# print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEX_TABLE_DIR + \"all_nb-deblurring.tex\", \"w\") as f:\n",
    "    f.write(latex)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2a8e9b59df830d48e9f72a82c46cdc1119b3ef257c7d0eadd36484a360f3166"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('masters-proj-eval': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
